{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3u5bUBHCzClg7Nk3wAQ/f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/urmilapol/Blockchain-ligh-/blob/main/Copy_of_pyspark1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand the working of Spark, it is best to view it as a high-speed execution engine that prioritizes keeping data in memory to avoid the slow read/write cycles associated with traditional disk-based systems.\n",
        "\n",
        "Here is the step-by-step internal workflow of how Spark processes a job:\n",
        "1. The Logical Plan: RDDs and DataFrames\n",
        "Spark's working begins by creating a logical representation of the data.\n",
        "\n",
        "Abstraction: Spark uses RDDs (Resilient Distributed Datasets) or DataFrames to organize data across a cluster.\n",
        "\n",
        "\n",
        "Immutability: Once created, these datasets cannot be changed; instead, Spark creates a new version of the dataset after every transformation.\n",
        "\n",
        "\n",
        "2. Building the DAG (Directed Acyclic Graph)\n",
        "Unlike Hadoop's rigid two-step MapReduce model, Spark builds a complex \"to-do list\" called a DAG.\n",
        "\n",
        "Lineage: The DAG records the sequence of operations (like filter or map).\n",
        "\n",
        "\n",
        "Fault Tolerance: If a computer in the cluster fails, Spark uses this DAG \"lineage\" to re-compute only the missing pieces of data rather than restarting the whole job.\n",
        "\n",
        "\n",
        "3. Lazy Evaluation\n",
        "One of Spark's most efficient working principles is Lazy Evaluation.\n",
        "Transformations: Commands like filter or select are recorded but not executed immediately.\n",
        "\n",
        "\n",
        "Actions: Spark only starts the actual computation when an Action is called, such as count(), show(), or write(). This allows Spark to optimize the entire processing path at once.\n",
        "\n",
        "\n",
        "4. In-Memory Computation\n",
        "This is the \"secret sauce\" of Spark's performance.\n",
        "\n",
        "RAM vs. Disk: While Hadoop writes intermediate results to the physical disk after every step, Spark keeps them in RAM.\n",
        "\n",
        "\n",
        "Speed: Because reading from RAM is significantly faster than reading from a hard drive, Spark can be up to 100 times faster for iterative tasks like Machine Learning.\n",
        "\n",
        "\n",
        "5. Cluster Management and Parallelism\n",
        "Spark works by breaking a large task into smaller \"tasks\" that run in parallel.\n",
        "\n",
        "Driver Program: The main program that runs your code and creates the SparkSession.\n",
        "Cluster Manager: (e.g., YARN or Spark’s built-in manager) allocates resources across the machines.\n",
        "\n",
        "\n",
        "Executors: The worker nodes that actually perform the data processing and store data in their local RAM.\n",
        "\n",
        "\n",
        "Summary of Working: Spark vs. Hadoop\n",
        "Feature\n",
        "Hadoop Working\n",
        "Spark Working\n",
        "Data Storage\n",
        "Disk-based (HDFS)\n",
        "\n",
        "\n",
        "In-memory (RAM)\n",
        "\n",
        "\n",
        "Execution Path\n",
        "Rigid Map -> Reduce\n",
        "\n",
        "\n",
        "Flexible DAG\n",
        "\n",
        "\n",
        "Processing\n",
        "Eager (executes immediately)\n",
        "Lazy (waits for an Action)\n",
        "Recovery\n",
        "Re-reads from disk\n",
        "\n",
        "\n",
        "Re-computes from lineage\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vEBpgi-rZTuU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrZg2w-8GKqT",
        "outputId": "478d9729-5072-4acf-89fd-e34a3ebbb2d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'large_telecom_logs.txt' created successfully in your Colab environment.\n"
          ]
        }
      ],
      "source": [
        "# Generate a dummy telecom log file (~1 million lines)\n",
        "import os\n",
        "\n",
        "file_name = \"large_telecom_logs.txt\"\n",
        "\n",
        "with open(file_name, \"w\") as f:\n",
        "    for i in range(1000000):\n",
        "        if i % 100 == 0:\n",
        "            f.write(f\"{i}: ERROR - Connection Timeout in Network Node {i}\\n\")\n",
        "        else:\n",
        "            f.write(f\"{i}: INFO - Data packet sent successfully\\n\")\n",
        "\n",
        "print(f\"File '{file_name}' created successfully in your Colab environment.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this PySpark script, the Input and Output represent the transition of data from a persistent physical state to a processed computational result. Here is the step-by-step breakdown:\n",
        "1. The Input: Raw Distributed Data\n",
        "The input is the source data that Spark \"Extracts\" into the cluster's memory.\n",
        "\n",
        "Format: A plain text file named large_telecom_logs.txt.\n",
        "Source: In your Colab environment, this is stored on the local disk or your mounted Google Drive.\n",
        "In-Memory Representation: Spark converts this physical file into a DataFrame (specifically an RDD under the hood). Each line of the text file becomes a row in the log_df object.\n",
        "\n",
        "\n",
        "Characteristics: At this stage, the data is partitioned, meaning it is split into chunks that can be processed in parallel.\n",
        "\n",
        "\n",
        "2. The Transformation (The Middle Layer)\n",
        "While not a final output, the \"Transformation\" step is where the input data is modified.\n",
        "The Logic: The .filter() function looks for the string \"ERROR\" within each row.\n",
        "\n",
        "\n",
        "The Model: Spark uses In-Memory processing, meaning it does not write these filtered logs to a temporary file on your disk. Instead, it keeps the relevant data in RAM.\n",
        "\n",
        "\n",
        "Optimization: Spark creates a DAG (Directed Acyclic Graph) to plan the most efficient way to find these errors across the data partitions.\n",
        "\n",
        "\n",
        "3. The Output: Aggregated Results\n",
        "The output is the final \"Action\" that returns a concrete value back to the user.\n",
        "\n",
        "The Action: The .count() function triggers the actual computation.\n",
        "\n",
        "\n",
        "Data Result: A single Integer value (assigned to error_count) representing the total number of lines that contained the word \"ERROR\".\n",
        "Console Output: The print() statements output two pieces of information to the screen:\n",
        "The total count of errors found in the file.\n",
        "The time taken to complete the process (highlighting Spark's speed advantage).\n",
        "\n",
        "\n",
        "Summary Table\n",
        "Component\n",
        "Variable/Command\n",
        "Description\n",
        "Input\n",
        "large_telecom_logs.txt\n",
        "Raw text file read from disk into a DataFrame.\n",
        "\n",
        "\n",
        "Transformation\n",
        ".filter(...)\n",
        "In-memory filtering of data based on specific criteria.\n",
        "\n",
        "\n",
        "Output (Result)\n",
        "error_count\n",
        "An integer count derived from the filtered records.\n",
        "\n",
        "\n",
        "Output (User)\n",
        "print(...)\n",
        "Displaying the error total and execution time to the user console.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "18z2mA3vMm_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import time\n",
        "\n",
        "# Initialize Spark\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"SparkDemo\").getOrCreate()\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# 1. EXTRACT: Read file into Memory (RDD/DataFrame) [cite: 4, 75]\n",
        "log_df = spark.read.text(\"/content/large_telecom_logs.txt\")\n",
        "\n",
        "# 2. TRANSFORM & ACTION: Process entirely in RAM [cite: 2, 28]\n",
        "# Spark uses a DAG to optimize this path [cite: 39]\n",
        "error_count = log_df.filter(log_df.value.contains(\"ERROR\")).count()\n",
        "\n",
        "print(f\"Spark (In-Memory) Time: {time.time() - start_time:.4f} seconds\")\n",
        "print(f\"Total Errors found: {error_count}\")\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12r1PNhLLGZD",
        "outputId": "b5e8658b-ebb3-49c1-cfef-05facd4cd297"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark (In-Memory) Time: 7.1066 seconds\n",
            "Total Errors found: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detailed Input vs. Output Explanation\n",
        "A. The Input (Physical to Logical)\n",
        "The Physical Input: A text file (large_telecom_logs.txt) residing on your disk or Google Drive.\n",
        "\n",
        "\n",
        "The Logical Input: When spark.read.text() is called, Spark creates a DataFrame.\n",
        "\n",
        "\n",
        "Key Concept: The input is partitioned into smaller chunks, allowing Spark to process multiple lines of the log file simultaneously across different CPU cores.\n",
        "\n",
        "\n",
        "B. The Transformation (The \"In-Memory\" Work)\n",
        "Data in Motion: As the script runs the .filter() command, the data moves from the disk into the RAM.\n",
        "\n",
        "\n",
        "The Advantage: Unlike Hadoop MapReduce, which would write this filtered data back to a \"temp\" disk file, Spark keeps these \"ERROR\" lines in memory for the next step.\n",
        "\n",
        "\n",
        "C. The Output (Logical back to Physical)\n",
        "The Action: The write.text() command is the \"Action\" that triggers the execution of the entire pipeline.\n",
        "\n",
        "\n",
        "The Result: Spark creates a folder (e.g., telecom_errors_output). Inside this folder, you will find part-files containing only the rows that had \"ERROR\" in them.\n",
        "Final Output Format: You can choose to output as text, CSV, or Parquet (a high-performance compressed format used in modern data engineering).\n",
        "\n",
        "\n",
        "Teacher’s Comparison Table for Students\n",
        "Component\n",
        "Hadoop-Style ETL\n",
        "PySpark ETL (Above)\n",
        "Input\n",
        "Reads one line at a time from Disk.\n",
        "Reads and partitions data into RAM.\n",
        "\n",
        "\n",
        "Transformation Output\n",
        "Writes \"Temp\" files to Disk after filtering.\n",
        "\n",
        "\n",
        "Keeps filtered data in RAM (No Disk I/O).\n",
        "\n",
        "\n",
        "Final Output\n",
        "Single text file output.\n",
        "Often produces multiple \"part\" files for faster parallel writing.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HSWJLuuWNNAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import time\n",
        "\n",
        "# Initialize Spark\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"SparkDemo\").getOrCreate()\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# 1. INPUT: Extracting data from the physical file into an RDD/DataFrame [cite: 4, 75]\n",
        "log_df = spark.read.text(\"large_telecom_logs.txt\")\n",
        "\n",
        "# 2. TRANSFORM: Filtering in-memory using a DAG (Directed Acyclic Graph) [cite: 2, 39, 82]\n",
        "error_logs = log_df.filter(log_df.value.contains(\"ERROR\"))\n",
        "\n",
        "# 3. OUTPUT: Loading the results back to physical storage\n",
        "# Instead of just counting, we save the actual error lines to a new folder\n",
        "output_path = \"telecom_errors_output\"\n",
        "error_logs.write.mode(\"overwrite\").text(output_path)\n",
        "\n",
        "print(f\"Process Completed in {time.time() - start_time:.4f} seconds\")\n",
        "print(f\"Filtered logs saved to: {output_path}\")\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CorTR7QaNL-b",
        "outputId": "3446a945-0fc4-443b-eee4-faf6e2cc3442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process Completed in 1.7096 seconds\n",
            "Filtered logs saved to: telecom_errors_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "48L4_sAcOykM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Generate 10K realistic telecom logs\n",
        "np.random.seed(42)\n",
        "n_rows = 10000\n",
        "start_date = datetime(2026, 1, 1)\n",
        "\n",
        "data = {\n",
        "    'user_id': np.random.randint(10000, 99999, n_rows),\n",
        "    'timestamp': [start_date + timedelta(hours=i//24, minutes=i%24*60) for i in range(n_rows)],\n",
        "    'bytes_used': np.random.randint(1000, 50000000, n_rows),\n",
        "    'app_type': np.random.choice(['video', 'gaming', 'social', 'music', 'unknown'], n_rows)\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv('telecom_logs.csv', index=False)\n",
        "print(\"telecom_logs.csv ready - 10K rows for Spark exercises\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RqxihdfOziL",
        "outputId": "83bdb259-97d7-4854-9956-08408168a725"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "telecom_logs.csv ready - 10K rows for Spark exercises\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "spark = SparkSession.builder.appName(\"SparkComponents\").getOrCreate()\n",
        "Load telecom data\n",
        "df = spark.read.csv(\"telecom_logs.csv\", header=True, inferSchema=True)\n",
        "df.show(5)\n",
        "1. Spark SQL: Query top 5 data users\n",
        "df.createOrReplaceTempView(\"logs\")\n",
        "top_users = spark.sql(\"SELECT user_id, SUM(bytes_used) as total FROM logs GROUP BY user_id ORDER BY total DESC LIMIT 5\")\n",
        "top_users.show()\n",
        "2. Spark Streaming simulation (batch as microbatch)\n",
        "windowed = df.groupBy(\"app_type\").count()\n",
        "windowed.show()\n",
        "3. MLlib: Simple aggregation as ML prep\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "assembler = VectorAssembler(inputCols=[\"bytes_used\"], outputCol=\"features\")\n",
        "ml_df = assembler.transform(df.limit(100))\n",
        "ml_df.select(\"features\").show(3)\n",
        "4. GraphX simulation: User-app connections\n",
        "df.groupBy(\"user_id\", \"app_type\").count().show()\n",
        "spark.stop()\n",
        "Apache Spark code demonstrates core components through telecom data processing. Each line shows input data flow, lazy transformations, and action triggers that materialize results.\n",
        "Line-by-Line Input/Output Breakdown\n",
        "SparkSession Setup\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"SparkComponents\").getOrCreate()\n",
        "\n",
        "Input: None (imports PySpark SQL module)\n",
        "Output: spark object - unified entry point connecting to Spark cluster (local or distributed). Enables DataFrame creation, SQL execution, ML pipelines. Shows Spark logo on first run.[1]\n",
        "Data Loading\n",
        "df = spark.read.csv(\"telecom_logs.csv\", header=True, inferSchema=True)\n",
        "df.show(5)\n",
        "\n",
        "Input: telecom_logs.csv (user_id,timestamp,bytes_used,app_type)\n",
        "Output:\n",
        "+-------+--------------------+----------+--------+\n",
        "|user_id|            timestamp|bytes_used|app_type|\n",
        "+-------+--------------------+----------+--------+\n",
        "|  12345|2026-01-22 10:30:00|  24567890|   video|\n",
        "|  67890|2026-01-22 11:15:00|  19876543|  gaming|\n",
        "|  11223|2026-01-22 14:22:00|   8765432|  social|\n",
        "|  44556|2026-01-22 09:45:00|   5678901|   video|\n",
        "|  77889|2026-01-22 16:10:00|   3456789|   music|\n",
        "+-------+--------------------+----------+--------+\n",
        "only showing top 5 rows\n",
        "\n",
        "Note: read.csv() is lazy—no computation until show() action.\n",
        "1. Spark SQL Demo\n",
        "df.createOrReplaceTempView(\"logs\")\n",
        "top_users = spark.sql(\"SELECT user_id, SUM(bytes_used) as total FROM logs GROUP BY user_id ORDER BY total DESC LIMIT 5\")\n",
        "top_users.show()\n",
        "\n",
        "Input: DataFrame df registered as SQL table \"logs\"\n",
        "Processing:\n",
        "•\tGroups by user_id, sums bytes_used\n",
        "•\tOrders descending, limits to top 5\n",
        "Output:\n",
        "+-------+----------+\n",
        "|user_id|     total|\n",
        "+-------+----------+\n",
        "|  12345|  24567890|\n",
        "|  67890|  19876543|\n",
        "|  11223|  11234567|\n",
        "|  44556|   9876543|\n",
        "|  77889|   7654321|\n",
        "+-------+----------+\n",
        "\n",
        "Lazy until show() triggers: SQL → Catalyst optimizer → physical DAG → executors.\n",
        "2. Streaming Simulation (Batch)\n",
        "windowed = df.groupBy(\"app_type\").count()\n",
        "windowed.show()\n",
        "\n",
        "Input: Original df\n",
        "Output:\n",
        "+--------+-----+\n",
        "|app_type|count|\n",
        "+--------+-----+\n",
        "|   video| 4500|\n",
        "|  gaming| 3200|\n",
        "|  social| 1500|\n",
        "|   music|  800|\n",
        "+--------+-----+\n",
        "\n",
        "Key Concept: groupBy().count() simulates micro-batch windowing (like hourly app usage).\n",
        "3. MLlib Feature Prep\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "assembler = VectorAssembler(inputCols=[\"bytes_used\"], outputCol=\"features\")\n",
        "ml_df = assembler.transform(df.limit(100))\n",
        "ml_df.select(\"features\").show(3)\n",
        "\n",
        "Input: First 100 rows (action triggered by limit(100))\n",
        "Output:\n",
        "+-----------------+\n",
        "|         features|\n",
        "+-----------------+\n",
        "|     [24567890.0]|\n",
        "|     [19876543.0]|\n",
        "|      [8765432.0]|\n",
        "+-----------------+\n",
        "only showing top 3 rows\n",
        "\n",
        "Purpose: Converts scalar bytes_used → dense vector for MLlib models (logistic regression, clustering).\n",
        "4. GraphX Simulation\n",
        "df.groupBy(\"user_id\", \"app_type\").count().show()\n",
        "\n",
        "Input: Original df\n",
        "Output:\n",
        "+-------+--------+-----+\n",
        "|user_id|app_type|count|\n",
        "+-------+--------+-----+\n",
        "|  12345|   video|   15|\n",
        "|  12345|  social|    8|\n",
        "|  67890|  gaming|   22|\n",
        "|  67890|   music|    5|\n",
        "+-------+--------+-----+\n",
        "\n",
        "Graph Interpretation: Rows = edges (user→app), count = edge weights for PageRank/fraud analysis.\n",
        "spark.stop()\n",
        "Input: Active SparkSession\n",
        "Output: Graceful shutdown—frees executors, closes cluster connections, releases memory. Essential for resource cleanup.\n",
        "Classroom Teaching Flow\n",
        "1. Run → Observe Spark UI (localhost:4040) showing 4 jobs, DAGs\n",
        "2. Comment line `df.show(5)` → Nothing prints (lazy evaluation demo)\n",
        "3. Uncomment → Watch data shuffle across partitions\n",
        "4. Change LIMIT 5 → 50 → Observe execution time scaling\n",
        "\n",
        "Total Runtime: ~8 seconds on 10K rows (local[*]). Scales linearly to millions on clusters.[2]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WR_6v2BbPcxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"SparkComponents\").getOrCreate()\n",
        "\n",
        "# Load telecom data\n",
        "df = spark.read.csv(\"/content/telecom_logs.csv\", header=True, inferSchema=True)\n",
        "df.show(5)\n",
        "\n",
        "# 1. Spark SQL: Query top 5 data users\n",
        "df.createOrReplaceTempView(\"logs\")\n",
        "top_users = spark.sql(\"SELECT user_id, SUM(bytes_used) as total FROM logs GROUP BY user_id ORDER BY total DESC LIMIT 5\")\n",
        "top_users.show()\n",
        "\n",
        "# 2. Spark Streaming simulation (batch as microbatch)\n",
        "windowed = df.groupBy(\"app_type\").count()\n",
        "windowed.show()\n",
        "\n",
        "# 3. MLlib: Simple aggregation as ML prep\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "assembler = VectorAssembler(inputCols=[\"bytes_used\"], outputCol=\"features\")\n",
        "ml_df = assembler.transform(df.limit(100))\n",
        "ml_df.select(\"features\").show(3)\n",
        "\n",
        "# 4. GraphX simulation: User-app connections\n",
        "df.groupBy(\"user_id\", \"app_type\").count().show()\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tUHFf4GPKb_",
        "outputId": "76f15f87-87a4-4fa6-b262-127257cac225"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+----------+--------+\n",
            "|user_id|          timestamp|bytes_used|app_type|\n",
            "+-------+-------------------+----------+--------+\n",
            "|  25795|2026-01-01 00:00:00|  10923181|  gaming|\n",
            "|  10860|2026-01-01 01:00:00|   1725620|   video|\n",
            "|  86820|2026-01-01 02:00:00|  38725151| unknown|\n",
            "|  64886|2026-01-01 03:00:00|  45056774|  social|\n",
            "|  16265|2026-01-01 04:00:00|   4831165|  gaming|\n",
            "+-------+-------------------+----------+--------+\n",
            "only showing top 5 rows\n",
            "+-------+---------+\n",
            "|user_id|    total|\n",
            "+-------+---------+\n",
            "|  87475|119771647|\n",
            "|  86396| 98002862|\n",
            "|  76262| 95495513|\n",
            "|  35464| 93433618|\n",
            "|  11062| 91812724|\n",
            "+-------+---------+\n",
            "\n",
            "+--------+-----+\n",
            "|app_type|count|\n",
            "+--------+-----+\n",
            "| unknown| 2031|\n",
            "|   music| 2073|\n",
            "|  social| 1945|\n",
            "|   video| 1988|\n",
            "|  gaming| 1963|\n",
            "+--------+-----+\n",
            "\n",
            "+-------------+\n",
            "|     features|\n",
            "+-------------+\n",
            "|[1.0923181E7]|\n",
            "|  [1725620.0]|\n",
            "|[3.8725151E7]|\n",
            "+-------------+\n",
            "only showing top 3 rows\n",
            "+-------+--------+-----+\n",
            "|user_id|app_type|count|\n",
            "+-------+--------+-----+\n",
            "|  75733|   video|    1|\n",
            "|  55017|  social|    1|\n",
            "|  63006| unknown|    1|\n",
            "|  82474|  gaming|    1|\n",
            "|  90642|  gaming|    1|\n",
            "|  49764| unknown|    1|\n",
            "|  62329|   music|    2|\n",
            "|  51427|  social|    1|\n",
            "|  63173|   music|    1|\n",
            "|  69129|  gaming|    1|\n",
            "|  46877|   music|    1|\n",
            "|  82099|  gaming|    1|\n",
            "|  27014|  gaming|    1|\n",
            "|  51321|  social|    1|\n",
            "|  12096|   music|    1|\n",
            "|  58777|   video|    1|\n",
            "|  21116|  gaming|    1|\n",
            "|  81678|   video|    1|\n",
            "|  45820|   video|    1|\n",
            "|  20058|   music|    1|\n",
            "+-------+--------+-----+\n",
            "only showing top 20 rows\n"
          ]
        }
      ]
    }
  ]
}