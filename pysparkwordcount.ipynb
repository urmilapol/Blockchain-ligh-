{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJooxWpZlKdibcU1y+J27L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/urmilapol/Blockchain-ligh-/blob/main/pysparkwordcount.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Setup: Install PySpark:\n",
        "\n",
        "!pip install pyspark -q: This command installs the PySpark library quietly, which is necessary to use Spark functionalities in Python.\n",
        "from pyspark.sql import SparkSession: Imports the SparkSession class, the entry point to programming Spark with the Dataset and DataFrame API.\n",
        "import re: Imports the regular expression module, used for text processing.\n",
        "2. Initialize Spark Session:\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"WordCount\").getOrCreate(): This line creates or gets an existing SparkSession.\n",
        ".master(\"local[*]\"): Configures Spark to run locally, using all available CPU cores on your machine.\n",
        ".appName(\"WordCount\"): Sets a name for your Spark application.\n",
        "sc = spark.sparkContext: Retrieves the SparkContext from the SparkSession. The SparkContext is the main entry point for Spark's RDD (Resilient Distributed Dataset) API.\n",
        "3. INPUT: Create a dummy text file:\n",
        "\n",
        "with open(\"sample_text.txt\", \"w\") as f: f.write(\"Hadoop is good for storage. Spark is good for speed. Spark and Hadoop work together.\"): This creates a simple text file named sample_text.txt with some example content that will be used for the word count.\n",
        "4. EXTRACT: Read the file into an RDD:\n",
        "\n",
        "text_rdd = sc.textFile(\"sample_text.txt\"): This reads the content of sample_text.txt into a Spark RDD. Each line of the file becomes an element in the RDD.\n",
        "5. TRANSFORM: The working logic:\n",
        "\n",
        "This section performs the actual word count using a series of transformations on the RDD:\n",
        ".flatMap(lambda line: re.findall(r'\\w+', line.lower())): This is a transformation that first converts each line to lowercase (line.lower()) and then uses a regular expression (re.findall(r'\\w+', ...) ) to find all word characters. flatMap then flattens the list of words from each line into a single RDD of individual words.\n",
        ".map(lambda word: (word, 1)): This transformation takes each word and pairs it with the number 1, creating a (word, 1) tuple for each occurrence of a word.\n",
        ".reduceByKey(lambda a, b: a + b): This transformation groups all tuples by their key (the word) and then applies a reduction function (summing the values) to get the total count for each unique word.\n",
        "6. OUTPUT (Action): Trigger the computation and print results:\n",
        "\n",
        "results = word_counts.collect(): collect() is an action that triggers the execution of all the previous transformations. It brings all the results from the distributed Spark environment back to the driver program (your Colab notebook) as a list of (word, count) tuples.\n",
        "The print statements then iterate through these results and display each word along with its count.\n",
        "7. CLEANUP:\n",
        "\n",
        "spark.stop(): This gracefully stops the SparkSession and releases all associated resources."
      ],
      "metadata": {
        "id": "dmIT0KZkdE0B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This script extracts text, transforms it into individual words, counts the frequency of each word, and loads the results back to storage.**"
      ],
      "metadata": {
        "id": "10bXYwIqeB_L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClD35deAc4Jb",
        "outputId": "37bd2158-5529-4623-9e86-e2ac4f710726"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Word Count Results ---\n",
            "hadoop: 2\n",
            "good: 2\n",
            "for: 2\n",
            "storage: 1\n",
            "speed: 1\n",
            "and: 1\n",
            "work: 1\n",
            "is: 2\n",
            "spark: 2\n",
            "together: 1\n"
          ]
        }
      ],
      "source": [
        "# 1. Setup: Install PySpark\n",
        "!pip install pyspark -q\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "import re\n",
        "\n",
        "# 2. Initialize Spark Session\n",
        "# 'local[*]' tells Spark to use all available CPU cores on the machine [cite: 4]\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"WordCount\").getOrCreate()\n",
        "sc = spark.sparkContext # Access SparkContext for RDD operations\n",
        "\n",
        "# 3. INPUT: Create a dummy text file\n",
        "with open(\"sample_text.txt\", \"w\") as f:\n",
        "    f.write(\"Hadoop is good for storage. Spark is good for speed. Spark and Hadoop work together.\")\n",
        "\n",
        "# 4. EXTRACT: Read the file into an RDD [cite: 4, 100]\n",
        "text_rdd = sc.textFile(\"sample_text.txt\")\n",
        "\n",
        "# 5. TRANSFORM: The working logic\n",
        "# We use flatMap to split lines into words, map to create pairs, and reduceByKey to sum\n",
        "word_counts = text_rdd.flatMap(lambda line: re.findall(r'\\w+', line.lower())) \\\n",
        "                     .map(lambda word: (word, 1)) \\\n",
        "                     .reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# 6. OUTPUT (Action): Trigger the computation and print results\n",
        "results = word_counts.collect()\n",
        "\n",
        "print(\"--- Word Count Results ---\")\n",
        "for word, count in results:\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "# 7. CLEANUP\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand WordCount, think of it as a distributed assembly line. Instead of one person counting every word in a huge book, Spark splits the book into pages and gives them to different workers to count simultaneously.\n",
        "Here is the simple data flow of how Spark processes the sentence: \"Spark is fast. Hadoop is reliable.\"\n",
        "Step 1: The Input (Loading Data)\n",
        "Spark reads the raw text file from storage (like your Local Disk or HDFS) into an RDD or DataFrame.\n",
        "\n",
        "Data State: [\"Spark is fast. Hadoop is reliable.\"]\n",
        "Step 2: Tokenization (Splitting)\n",
        "The flatMap operation breaks the sentences into individual words.\n",
        "\n",
        "Data State: [\"Spark\", \"is\", \"fast\", \"Hadoop\", \"is\", \"reliable\"]\n",
        "Step 3: Mapping (Key-Value Pairs)\n",
        "The map operation turns each word into a pair with the number 1. This is like giving each word a \"vote\".\n",
        "\n",
        "Data State: [(\"Spark\", 1), (\"is\", 1), (\"fast\", 1), (\"Hadoop\", 1), (\"is\", 1), (\"reliable\", 1)]\n",
        "Step 4: Shuffling (Grouping)\n",
        "Spark moves all identical words to the same worker node. This is the only part where data might move across the network.\n",
        "\n",
        "Data State: * Worker A: (\"is\", 1), (\"is\", 1)\n",
        "Worker B: (\"Spark\", 1), (\"Hadoop\", 1), (\"fast\", 1), (\"reliable\", 1)\n",
        "Step 5: Reducing (Aggregating)\n",
        "The reduceByKey operation sums the \"1s\" for each word.\n",
        "\n",
        "Data State: [(\"is\", 2), (\"Spark\", 1), (\"Hadoop\", 1), (\"fast\", 1), (\"reliable\", 1)]\n",
        "Summary Table for Students\n",
        "Stage\n",
        "Operation\n",
        "What happens to the data?\n",
        "Input\n",
        "textFile\n",
        "Data is loaded into RAM.\n",
        "\n",
        "\n",
        "Split\n",
        "flatMap\n",
        "Sentences become individual words.\n",
        "Map\n",
        "map\n",
        "Each word is paired with a count of 1.\n",
        "\n",
        "\n",
        "Reduce\n",
        "reduceByKey\n",
        "Similar words are grouped and their counts are summed.\n",
        "\n",
        "\n",
        "Result\n",
        "collect\n",
        "The final list is sent back to the Driver program.\n",
        "\n",
        "\n",
        "\n",
        "Why is this \"Spark Style\"?\n",
        "In a Hadoop MapReduce flow, the data would be written to the physical disk after the Map and Shuffle phases. In Spark, this entire flow happens in-memory (RAM), which is why the word count finishes in seconds rather than minutes.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Z6MiPhdofCXP"
      }
    }
  ]
}