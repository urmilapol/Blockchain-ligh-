{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBy9lG8c3QNwNL8uslO79Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/urmilapol/Blockchain-ligh-/blob/main/pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apache Spark provides a unified engine for large-scale data processing, overcoming Hadoop's disk-based limitations through in-memory computation and versatile APIs. These lecture notes distill the architecture from the provided resource, structured for your data engineering classes with MSc students.[1]\n",
        "Origin and Key Features\n",
        "Spark originated in 2009 at UC Berkeley's AMPLab to address Hadoop MapReduce inefficiencies, becoming an Apache top-level project by 2014. Core strengths include in-memory computing for 100x speedups, unified analytics across batch/streaming/ML, polyglot support (Scala, Java, Python, R), and fault-tolerant RDDs with lazy evaluation via DAG execution.[2][1]\n",
        "Core Architecture Components\n",
        "Spark follows a master-worker model where the driver coordinates tasks across a cluster managed by YARN, Standalone, Mesos, or Kubernetes. Key elements include:\n",
        "â€¢\tDriver Program: Runs main() with SparkContext; builds DAGs, schedules stages/tasks, monitors execution.[3][1]\n",
        "â€¢\tCluster Manager: Allocates CPU/memory resources to worker nodes.[1]\n",
        "â€¢\tExecutors: Worker processes on nodes; execute tasks, cache data in memory/disk, handle shuffles.[4][3]\n",
        "â€¢\tWorker Nodes: Host multiple executors for parallel processing.[1]\n",
        "Fundamental Abstractions\n",
        "RDDs enable immutable, distributed data with lineage for recomputation on failure, while DAGs optimize transformation chains into stages. Execution flow: submit app â†’ driver builds/optimizes DAG â†’ scheduler assigns tasks â†’ executors process with data locality and speculative execution.[5][1]\n",
        "Libraries and Workloads\n",
        "Library\tWorkload\tExample\n",
        "Spark Core\tBatch processing, RDD APIs\tETL transformations[1]\n",
        "\n",
        "Spark SQL\tInteractive queries\tDataFrame analytics[1]\n",
        "\n",
        "Structured Streaming\tNear real-time\tKafka stream processing[1]\n",
        "\n",
        "MLlib\tMachine learning\tScalable classification/clustering[1]\n",
        "\n",
        "GraphX\tGraph analytics\tFraud detection on networks[1]\n",
        "\n",
        "\n",
        "Spark vs Alternatives\n",
        "Spark outperforms Hadoop in speed (in-memory vs disk) and versatility (real-time/ML support), though it requires more RAM. Compared to Hive, Spark handles diverse data types and low-latency tasks better via APIs beyond SQL.[1]\n",
        "For teaching, demo PySpark sessions: create SparkSession, load data, apply transformations, visualize DAG in Spark UIâ€”pair with telecom fraud exercises from prior notes.[1]\n",
        "\n"
      ],
      "metadata": {
        "id": "3Q2ajIUgXKxb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Hadoop and Spark address big data challenges through scalable processing and storage. This lecture covers their key use cases and scenarios, drawing from real-world industry applications.[1]\n",
        "Hadoop Use Cases\n",
        "Hadoop excels in batch processing for large, non-real-time datasets. It supports data warehousing by enabling analytical queries on massive volumes.[1]\n",
        "â€¢\tHealthcare and finance sectors use it for customer interaction analysis and recommendation systems.\n",
        "â€¢\tTelecommunication and e-commerce leverage scalable, fault-tolerant storage across clusters.\n",
        "â€¢\tMission-critical transactions in finance benefit from high availability and disaster recovery via data replication.[1]\n",
        "Spark Use Cases\n",
        "Spark handles versatile, real-time data processing with APIs like Spark Streaming. It supports machine learning via Spark MLlib and graph analytics via GraphX.[1]\n",
        "â€¢\tTelecommunication industry applies it for viewer engagement, ad targeting, and recommendations.\n",
        "â€¢\tFraud detection uses graph processing to uncover hidden patterns through nodes and relationships.\n",
        "â€¢\tProduction deployment of complex ML algorithms creates comprehensive data fabrics.[1]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "In the world of data engineering, the comparison between Apache Hadoop and Apache Spark is often framed as a competition, but they are more like complementary tools. While Hadoop is a comprehensive ecosystem for storage and processing, Spark is a high-performance engine specifically designed for speed and flexibility.1\n",
        "\n",
        "The primary technical difference is that Hadoop MapReduce is disk-based, whereas Apache Spark is memory-based.2\n",
        "\n",
        "________________________________________\n",
        "Key Comparison: Hadoop vs. Spark\n",
        "Feature\tApache Hadoop (MapReduce)\tApache Spark\n",
        "Primary Strength\tCost-effective storage & batch processing.\tHigh-speed, in-memory computation.\n",
        "Data Processing\tBatch only. Processes data in large, scheduled chunks.\tBatch + Streaming. Can process data in near real-time.\n",
        "Speed\tSlower (reads/writes to disk for every step).\tFast (up to 100x faster by keeping data in RAM).\n",
        "Storage\tHas its own file system (HDFS).\tNo native storage; usually runs on HDFS or S3.\n",
        "Fault Tolerance\tHigh (replicates data blocks across nodes).\tHigh (recomputes lost data using \"Lineage\").\n",
        "Ease of Use\tHarder; requires complex Java boilerplate.\tEasier; high-level APIs in Python, Scala, and SQL.\n",
        "________________________________________\n",
        "Architectural Differences\n",
        "1. Processing Model\n",
        "â€¢\tHadoop (MapReduce): Operates in a rigid two-step \"Map\" and \"Reduce\" phase.3 After every step, it must write the intermediate results to the physical disk.4 This makes it highly reliable but introduces significant I/O latency.\n",
        "\n",
        "\n",
        "â€¢\tSpark: Uses a DAG (Directed Acyclic Graph) execution engine.5 It analyzes the entire chain of data transformations and optimizes the execution plan before running it. It keeps intermediate data in RAM (Random Access Memory), avoiding the \"disk bottleneck.\"6\n",
        "\n",
        "\n",
        "2. Ecosystem and Components\n",
        "Hadoop is a suite of several components, whereas Spark is often treated as a replacement for only one of those components (MapReduce).7\n",
        "\n",
        "\n",
        "â€¢\tHadoop Ecosystem: Includes HDFS (Storage), YARN (Resource Management), and MapReduce (Processing).8\n",
        "\n",
        "\n",
        "â€¢\tSpark Ecosystem: Includes Spark SQL (for database queries), Spark Streaming (for real-time data), and MLlib (for machine learning).9\n",
        "\n",
        "\n",
        "3. Fault Tolerance\n",
        "â€¢\tHadoop achieves fault tolerance through replication.10 It writes three copies of every piece of data across different machines.\n",
        "\n",
        "\n",
        "â€¢\tSpark uses Resilient Distributed Datasets (RDDs).11 Instead of replicating data, it remembers the \"lineage\" (the sequence of instructions) used to create a dataset. If a machine fails, Spark simply re-runs those instructions on the original data to recover the lost part.12\n",
        "\n",
        "\n",
        "________________________________________\n",
        "Which one should you use?\n",
        "â€¢\tChoose Hadoop if you have massive amounts of \"cold\" data (petabytes) where storage cost is more important than speed, or if you are running long-running batch ETL (Extract, Transform, Load) jobs that don't need to finish in seconds.13\n",
        "\n",
        "\n",
        "â€¢\tChoose Spark if you need to process data in real-time (like fraud detection), if you are building Machine Learning models that require multiple passes over the same data, or if you want a faster, more modern development experience with Python (PySpark).14\n",
        "\n",
        "\n",
        "Note: Most modern data platforms use both. They use Hadoop (HDFS) to store the data cheaply and Spark to process it quickly.\n",
        "Would you like me to explain how Spark's \"lazy evaluation\" helps optimize these data pipelines?\n",
        "Hadoop vs Spark use cases\n",
        "This video provides a deep dive into the real-world scenarios and industrial use cases where data engineers choose between Hadoop and Spark for their pipelines.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Apache Spark is an open-source, distributed computing framework designed for fast, in-memory data processing at scale. It outperforms traditional MapReduce by processing data primarily in memory, enabling iterative algorithms and real-time analytics.[1][2]\n",
        "Core Architecture\n",
        "Spark operates on a master-worker model with a driver program coordinating tasks across a cluster. The SparkContext serves as the entry point, creating RDDs (Resilient Distributed Datasets) and managing job execution via a Directed Acyclic Graph (DAG) for lazy evaluation and optimization.[2][1]\n",
        "â€¢\tDriver converts user code into a DAG, schedules stages, and monitors progress.\n",
        "â€¢\tExecutors on worker nodes handle tasks, caching data in memory or disk for fault tolerance.\n",
        "â€¢\tCluster managers (Standalone, YARN, Kubernetes) allocate resources dynamically.[1]\n",
        "Key Components\n",
        "Spark Core provides foundational RDD APIs for resilient, distributed data manipulation with transformations (e.g., map, filter) and actions (e.g., count, collect). Higher-level libraries build on this for SQL (DataFrames), streaming, MLlib (machine learning), and GraphX (graph processing).[3][2]\n",
        "Component\tPurpose\tExample Use\n",
        "Spark Core\tRDDs, task scheduling, fault recovery\tBasic data transformations[3]\n",
        "\n",
        "Spark SQL\tStructured data queries\tDataFrame operations like SELECT[2]\n",
        "\n",
        "Spark Streaming\tReal-time data ingestion\tProcess live telecom streams[4]\n",
        "\n",
        "MLlib\tScalable ML algorithms\tFraud detection models[4]\n",
        "\n",
        "GraphX\tGraph analytics\tRelationship pattern detection[1]\n",
        "\n",
        "\n",
        "Use Cases and Advantages\n",
        "Spark excels in real-time processing (via Streaming API), batch analytics, and machine learning deployment, as seen in telecom ad targeting and fraud detection. Its in-memory computation yields 100x speedups over disk-based Hadoop for iterative tasks.[4][1]\n",
        "For your data engineering lectures, demonstrate with PySpark code: create a SparkSession, load sample data as DataFrame, apply transformations, and trigger an actionâ€”emphasizing DAG visualization in Spark UI.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mgZifU_HWUba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== COMPLETE STOCK P/E RATIO ETL PIPELINE =====\n",
        "# Video: https://www.youtube.com/watch?v=KAuIvccwbPY\n",
        "\n",
        "# 1. Initialize SparkSession (Driver Program Entry Point)\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when, avg, count\n",
        "\n",
        "# Create SparkSession - connects to cluster (local/4 nodes/serverless)\n",
        "# Use cluster: \"yarn\", \"k8s://...\", or omit for default\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"StockPEratioDemo\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"âœ… SparkSession created. Cluster ready for distributed processing.\")\n",
        "\n",
        "# 2. EXTRACT: Load CSV (Spark auto-partitions across workers)\n",
        "df_raw = spark.read \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .csv(\"/content/sample_data/stocks.csv\")  # Scales to HDFS/S3 paths\n",
        "\n",
        "print(\"ðŸ“Š Raw data schema:\")\n",
        "df_raw.printSchema()\n",
        "df_raw.show()\n",
        "\n",
        "# 3. TRANSFORM: Business logic (lazy evaluation - no execution yet)\n",
        "df_pe = df_raw.withColumn(\n",
        "    \"pe_ratio\",\n",
        "    col(\"Price\") / col(\"EPS\")\n",
        ").withColumn(\n",
        "    \"pe_category\",\n",
        "    when(col(\"pe_ratio\") < 15, \"Undervalued\")\n",
        "    .when(col(\"pe_ratio\") > 30, \"Overvalued\")\n",
        "    .otherwise(\"Fair\")\n",
        ")\n",
        "\n",
        "print(\"ðŸ”„ Transformation DAG built (lazy)\")\n",
        "\n",
        "# 4. LOAD/ACTION: Triggers execution across cluster\n",
        "print(\"ðŸš€ EXECUTING across cluster...\")\n",
        "df_result = df_pe.orderBy(col(\"pe_ratio\"))\n",
        "df_result.select(\"Symbol\", \"Price\", \"EPS\", \"pe_ratio\", \"pe_category\").show(truncate=False)\n",
        "\n",
        "# 5. Advanced: Aggregate analytics (real ETL use case)\n",
        "df_summary = df_pe.groupBy(\"pe_category\").agg(\n",
        "    avg(\"pe_ratio\").alias(\"avg_pe\"),\n",
        "    count(\"Symbol\").alias(\"stock_count\")\n",
        ").orderBy(\"pe_category\")\n",
        "print(\"ðŸ“ˆ Portfolio Summary:\")\n",
        "df_summary.show()\n",
        "\n",
        "# 6. Write results (complete ETL)\n",
        "df_result.coalesce(1).write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .csv(\"stocks_pe_analysis\")\n",
        "\n",
        "print(\"ðŸ’¾ Results written to stocks_pe_analysis/\")\n",
        "\n",
        "# 7. Cleanup\n",
        "spark.stop()\n",
        "print(\"âœ… Spark cluster shutdown.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dfc1b60-40e8-40bb-c757-1441aa3d9660",
        "id": "B5RX3Nnzvyeq"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… SparkSession created. Cluster ready for distributed processing.\n",
            "ðŸ“Š Raw data schema:\n",
            "root\n",
            " |-- Symbol: string (nullable = true)\n",
            " |-- Price: double (nullable = true)\n",
            " |-- EPS: double (nullable = true)\n",
            "\n",
            "+------+-------+-----+\n",
            "|Symbol|  Price|  EPS|\n",
            "+------+-------+-----+\n",
            "|  AAPL|  175.5| 6.16|\n",
            "|  GOOG|2850.25|145.6|\n",
            "|  MSFT|  425.8| 11.8|\n",
            "|  TSLA|  245.3| 3.65|\n",
            "|  AMZN|  186.8| 4.12|\n",
            "|  META|  567.9|20.35|\n",
            "|  NVDA| 890.45|12.05|\n",
            "+------+-------+-----+\n",
            "\n",
            "ðŸ”„ Transformation DAG built (lazy)\n",
            "ðŸš€ EXECUTING across cluster...\n",
            "+------+-------+-----+------------------+-----------+\n",
            "|Symbol|Price  |EPS  |pe_ratio          |pe_category|\n",
            "+------+-------+-----+------------------+-----------+\n",
            "|GOOG  |2850.25|145.6|19.575892857142858|Fair       |\n",
            "|META  |567.9  |20.35|27.906633906633903|Fair       |\n",
            "|AAPL  |175.5  |6.16 |28.490259740259738|Fair       |\n",
            "|MSFT  |425.8  |11.8 |36.08474576271186 |Overvalued |\n",
            "|AMZN  |186.8  |4.12 |45.33980582524272 |Overvalued |\n",
            "|TSLA  |245.3  |3.65 |67.2054794520548  |Overvalued |\n",
            "|NVDA  |890.45 |12.05|73.89626556016597 |Overvalued |\n",
            "+------+-------+-----+------------------+-----------+\n",
            "\n",
            "ðŸ“ˆ Portfolio Summary:\n",
            "+-----------+------------------+-----------+\n",
            "|pe_category|            avg_pe|stock_count|\n",
            "+-----------+------------------+-----------+\n",
            "|       Fair|25.324262168012165|          3|\n",
            "| Overvalued| 55.63157415004384|          4|\n",
            "+-----------+------------------+-----------+\n",
            "\n",
            "ðŸ’¾ Results written to stocks_pe_analysis/\n",
            "âœ… Spark cluster shutdown.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.chaosgenius.io/blog/apache-spark-architecture/\n",
        "\n",
        "\n",
        "Apache Spark follows a master-slave (driver-worker) architecture for distributed data processing, ideal for data engineering pipelines and interviews.\n",
        "High-Level Architecture\n",
        "Spark operates with a Driver Program (master), Cluster Manager, and Executors (workers) on cluster nodes. Tasks represent the smallest unit: a Spark job divides into stages, stages into tasks executed in parallel.[2][1]\n",
        "Cluster Manager (Standalone, YARN, Mesos, Kubernetes) allocates CPU/memory resources dynamically, upscaling for faster completion.[1]\n",
        "Master-slave: Driver coordinates; executors process; fault-tolerant via recomputation.[1]\n",
        "Driver Responsibilities\n",
        "Driver runs the main Spark application (your PySpark code), converting user code into a Directed Acyclic Graph (DAG)â€”a one-way graph of operations without loops. Requests resources from Cluster Manager, schedules tasks on executors, tracks progress, and collects results.[1]\n",
        "Key interview point: Driver handles logical-to-physical plan translation via DAG Scheduler.[1]\n",
        "Cluster Manager Role\n",
        "Manages resource allocation (e.g., CPUs, memory) to executors, monitors availability, and assigns free resources. YARN (Yet Another Resource Negotiator) is common in Hadoop ecosystems.[2][1]\n",
        "Executors and Tasks\n",
        "Executors (worker processes on nodes) execute tasks, store data in memory/disk, with dedicated CPU cores and task slots. Each holds executor memory for caching.[1]\n",
        "Task Scheduler assigns tasks to executors post-DAG breakdown.[1]\n",
        "Job Execution Flow\n",
        "â€¢\tUser submits code â†’ Driver creates logical plan.\n",
        "â€¢\tDAG Scheduler: Logical â†’ physical plan, breaks job into stages (shuffle boundaries).\n",
        "â€¢\tTask Scheduler: Stages â†’ tasks assigned to executors.\n",
        "â€¢\tCluster Manager allocates resources.\n",
        "â€¢\tExecutors run tasks in parallel â†’ Driver aggregates results for storage/output.[1]\n",
        "\n"
      ],
      "metadata": {
        "id": "Sk0n1hnvUxGg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvBykNEPSOgp",
        "outputId": "37965d54-cb41-4d67-bd15-d3da7a26e87f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----+--------+\n",
            "|CourseName|fee |discount|\n",
            "+----------+----+--------+\n",
            "|Java      |4000|5       |\n",
            "|Python    |4600|10      |\n",
            "|Scala     |4100|15      |\n",
            "+----------+----+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import upper\n",
        "\n",
        "spark = SparkSession.builder.appName(\"DataTransformation\").getOrCreate()\n",
        "\n",
        "# Sample data: courses with fees and discounts\n",
        "data = [(\"Java\", 4000, 5), (\"Python\", 4600, 10), (\"Scala\", 4100, 15)]\n",
        "columns = [\"CourseName\", \"fee\", \"discount\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def to_upper(df):\n",
        "    return df.withColumn(\"CourseName\", upper(df.CourseName))\n",
        "\n",
        "def reduce_price(df, amount):\n",
        "    return df.withColumn(\"new_fee\", df.fee - amount)\n",
        "\n",
        "def apply_discount(df):\n",
        "    return df.withColumn(\"discounted_fee\", df.new_fee * (1 - df.discount / 100))\n",
        "\n",
        "# Apply chain\n",
        "result = df.transform(to_upper).transform(reduce_price, 1000).transform(apply_discount)\n",
        "result.select(\"CourseName\", \"discounted_fee\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtQV_VK2Sayi",
        "outputId": "b64c3a4e-6d09-44fa-bc6f-7ce49c46090b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------+\n",
            "|CourseName|discounted_fee|\n",
            "+----------+--------------+\n",
            "|      JAVA|        2850.0|\n",
            "|    PYTHON|        3240.0|\n",
            "|     SCALA|        2635.0|\n",
            "+----------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Apache Spark excels in real-world data processing through transformations like joins, aggregations, and window functions on large datasets. Here's another hands-on PySpark example focused on sales data aggregationâ€”a common ETL scenario for retail analytics that builds on the prior fee transformation demo**\n",
        "Sales Aggregation Example\n",
        "# This processes transactional sales data to compute daily revenue by product category, filtering invalid records and applying windowed ranking for top performers.\n",
        "This PySpark code sets up an ETL (Extract, Transform, Load) pipeline to process sales data, aggregate it by daily revenue per category, and then rank the categories within each day.\n",
        "\n"
      ],
      "metadata": {
        "id": "bWveGycCXS3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# It imports necessary classes and functions from pyspark.sql  which is a library for working with structured data in Spark. It includes DataFrame for various data operations, and Window for defining window functions.\n",
        "from pyspark.sql.functions import col, to_date, sum as spark_sum, rank, desc\n",
        "from pyspark.sql.window import Window\n",
        "#creates or retrieves a SparkSession, which is the entry point to Spark functionality\n",
        "spark = SparkSession.builder.appName(\"SalesAggregation\").getOrCreate()\n",
        "\n",
        "# Sample sales data (scale to CSV from Kaggle e-commerce datasets)\n",
        "#sales_data is a Python list of tuples representing raw sales records, and columns defines the schema for this data. df = spark.createDataFrame(sales_data, columns) then converts this Python data into a Spark DataFrame.\n",
        "sales_data = [\n",
        "    (\"2025-01-01\", \"Electronics\", 100, 2),\n",
        "    (\"2025-01-01\", \"Clothing\", 50, 5),\n",
        "    (\"2025-01-02\", \"Electronics\", 100, 1),\n",
        "    (\"2025-01-02\", \"Clothing\", 50, 3),\n",
        "    (\"2025-01-01\", \"Books\", 20, 10),  # Low price, high volume\n",
        "    (\"2025-01-03\", \"Books\", 20, 0)    # Invalid (zero qty)\n",
        "]\n",
        "\n",
        "columns = [\"sale_date\", \"category\", \"price\", \"quantity\"]\n",
        "df = spark.createDataFrame(sales_data, columns)\n",
        "\n",
        "# ETL Pipeline: Clean â†’ Transform â†’ Aggregate\n",
        "#converts the sale_date column from a string to a proper date type.\n",
        "df_clean = df.filter(col(\"quantity\") > 0).withColumn(\"sale_date\", to_date(col(\"sale_date\")))\n",
        "#aggregates the cleaned data: groups by sale_date and category, and calculates the total revenue for each combination. calculates the total revenue for each group by multiplying price and quantity and summing them up.\n",
        "df_agg = df_clean.groupBy(\"sale_date\", \"category\").agg(spark_sum(col(\"price\") * col(\"quantity\")).alias(\"revenue\"))\n",
        "\n",
        "# Window function for ranking top categories per day   This section calculates the rank of each category's revenue within each day\n",
        "#defines a window specification. It partitions the data by sale_date (meaning ranks are calculated independently for each day) and orders the results within each partition by revenue in descending order.\n",
        "#applies this window function to df_agg, creating a new column named rank that assigns a rank to each category based on its revenue within its respective day.\n",
        "window_spec = Window.partitionBy(\"sale_date\").orderBy(desc(\"revenue\"))\n",
        "df_ranked = df_agg.withColumn(\"rank\", rank().over(window_spec))\n",
        "#displays the final df_ranked DataFrame, ordered by sale_date and then by revenue in descending order, showing the top-performing categories for each day.\n",
        "\n",
        "df_ranked.orderBy(\"sale_date\", desc(\"revenue\")).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnZVdqdpXXWU",
        "outputId": "3e3fc96a-2b51-485b-f1ed-2b5e7dc6c674"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-------+----+\n",
            "|sale_date |category   |revenue|rank|\n",
            "+----------+-----------+-------+----+\n",
            "|2025-01-01|Clothing   |250    |1   |\n",
            "|2025-01-01|Electronics|200    |2   |\n",
            "|2025-01-01|Books      |200    |2   |\n",
            "|2025-01-02|Clothing   |150    |1   |\n",
            "|2025-01-02|Electronics|100    |2   |\n",
            "+----------+-----------+-------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The complete PySpark stock P/E ratio ETL pipeline demonstrates Spark's distributed computing from extract â†’ transform â†’ load. Each line builds toward parallel execution across a cluster while abstracting complexity from the developer.[1]"
      ],
      "metadata": {
        "id": "GbTsr3ATtoFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== COMPLETE STOCK P/E RATIO ETL PIPELINE =====\n",
        "# Video: https://www.youtube.com/watch?v=KAuIvccwbPY\n",
        "\n",
        "# 1. Initialize SparkSession (Driver Program Entry Point)\n",
        "#: Imports core Spark SQL components. SparkSession creates the driver entry point; col() references DataFrame columns in expressions.\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when, avg, count\n",
        "\n",
        "# Create SparkSession - connects to cluster (local/4 nodes/serverless)\n",
        "# Use cluster: \"yarn\", \"k8s://...\", or omit for default\n",
        "#Purpose: Creates driver program connecting to cluster.\n",
        "#â€¢\tbuilder: Fluent API for configuration\n",
        "#â€¢\tappName: Identifies job in Spark UI/Cluster Manager\n",
        "#â€¢\tmaster(\"local[*]\"): Uses all CPU cores locally (change to \"yarn\" for cluster)\n",
        "#â€¢\tgetOrCreate(): Singleton pattern - reuses existing session or creates new one. Result: Driver ready to orchestrate workers.\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"StockPEratioDemo\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"âœ… SparkSession created. Cluster ready for distributed processing.\")\n",
        "\n",
        "\n",
        "\n",
        "# ETL Step 1 - Distributed read operation.\n",
        "#â€¢\tspark.read: DataFrameReader for structured formats\n",
        "#â€¢\theader=\"true\": First row becomes column names\n",
        "#â€¢\tinferSchema=\"true\": Auto-detects types (stringâ†’double)\n",
        "#â€¢\tKey: Spark partitions file across executors automatically\n",
        "#Lazy: No data loaded yet - just logical plan created.\n",
        "\n",
        "# 2. EXTRACT: Load CSV (Spark auto-partitions across workers)\n",
        "df_raw = spark.read \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .csv(\"/content/sample_data/stocks.csv\")  # Scales to HDFS/S3 paths\n",
        "\n",
        "print(\"ðŸ“Š Raw data schema:\")\n",
        "#Purpose: Actions trigger first execution.\n",
        "#â€¢\tprintSchema(): Shows inferred types (double for Price/EPS)\n",
        "#â€¢\tshow(): Materializes top 20 rows to driver console\n",
        "#Triggers: Catalyst optimizer + DAG execution across cluster.\n",
        "\n",
        "df_raw.printSchema()\n",
        "df_raw.show()\n",
        "\n",
        "\n",
        "\n",
        "#Purpose: ETL Step 2 - Business transformations (lazy).\n",
        "#â€¢\tFirst withColumn: Creates pe_ratio = Price Ã· EPS\n",
        "#â€¢\tSecond withColumn: Conditional logic chains (when/otherwise)\n",
        "#â€¢\tImmutable: Each returns new DataFrame\n",
        "#â€¢\tBroadcast: Formulas applied in parallel on each partition\n",
        "#DAG Built: Logical plan grows: read â†’ divide â†’ when() â†’ when()\n",
        "\n",
        "\n",
        "\n",
        "# 3. TRANSFORM: Business logic (lazy evaluation - no execution yet)\n",
        "df_pe = df_raw.withColumn(\n",
        "    \"pe_ratio\",\n",
        "    col(\"Price\") / col(\"EPS\")\n",
        ").withColumn(\n",
        "    \"pe_category\",\n",
        "    when(col(\"pe_ratio\") < 15, \"Undervalued\")\n",
        "    .when(col(\"pe_ratio\") > 30, \"Overvalued\")\n",
        "    .otherwise(\"Fair\")\n",
        ")\n",
        "\n",
        "print(\"ðŸ”„ Transformation DAG built (lazy)\")\n",
        "\n",
        "\n",
        "\n",
        "#Purpose: Triggers full execution pipeline.\n",
        "#â€¢\torderBy: Shuffle stage (data crosses partitions)\n",
        "#â€¢\tselect: Column projection optimization\n",
        "#â€¢\tshow(truncate=False): Full column display\n",
        "#Physical Plan: Driver â†’ DAG Scheduler â†’ Task Scheduler â†’ Executors.\n",
        "\n",
        "\n",
        "# 4. LOAD/ACTION: Triggers execution across cluster\n",
        "print(\"ðŸš€ EXECUTING across cluster...\")\n",
        "df_result = df_pe.orderBy(col(\"pe_ratio\"))\n",
        "df_result.select(\"Symbol\", \"Price\", \"EPS\", \"pe_ratio\", \"pe_category\").show(truncate=False)\n",
        "\n",
        "\n",
        "#Purpose: Real-world ETL aggregation pattern.\n",
        "#â€¢\tgroupBy: Shuffle by pe_category\n",
        "#â€¢\tagg: Multiple aggregations in single pass\n",
        "#â€¢\talias: Renames output columns\n",
        "#Optimization: Catalyst combines with prior operations.\n",
        "\n",
        "\n",
        "# 5. Advanced: Aggregate analytics (real ETL use case)\n",
        "df_summary = df_pe.groupBy(\"pe_category\").agg(\n",
        "    avg(\"pe_ratio\").alias(\"avg_pe\"),\n",
        "    count(\"Symbol\").alias(\"stock_count\")\n",
        ").orderBy(\"pe_category\")\n",
        "print(\"ðŸ“ˆ Portfolio Summary:\")\n",
        "df_summary.show()\n",
        "\n",
        "\n",
        "\n",
        "#Purpose: ETL Step 3 - Persist transformed data.\n",
        "#â€¢\tcoalesce(1): Single output file (remove for partitioned writes)\n",
        "#â€¢\tmode(\"overwrite\"): Replace existing output\n",
        "#â€¢\tDistributed Write: Executors write parallel partitions.\n",
        "\n",
        "# 6. Write results (complete ETL)\n",
        "df_result.coalesce(1).write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .csv(\"stocks_pe_analysis\")\n",
        "\n",
        "print(\"ðŸ’¾ Results written to stocks_pe_analysis/\")\n",
        "\n",
        "\n",
        "#Purpose: Release cluster resources (executors, memory).\n",
        "#Execution Timeline\n",
        "#1. Code written â†’ Logical Plan (lazy transformations)\n",
        "#2. .show()/.write() â†’ Catalyst Optimization â†’ Physical Plan\n",
        "#3. DAG Scheduler â†’ Stages â†’ Tasks â†’ Executors (parallel)\n",
        "#4. Results â†’ Driver â†’ Console/Storage\n",
        "\n",
        "# 7. Cleanup\n",
        "spark.stop()\n",
        "print(\"âœ… Spark cluster shutdown.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYuCszk7tq8K",
        "outputId": "5dfc1b60-40e8-40bb-c757-1441aa3d9660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… SparkSession created. Cluster ready for distributed processing.\n",
            "ðŸ“Š Raw data schema:\n",
            "root\n",
            " |-- Symbol: string (nullable = true)\n",
            " |-- Price: double (nullable = true)\n",
            " |-- EPS: double (nullable = true)\n",
            "\n",
            "+------+-------+-----+\n",
            "|Symbol|  Price|  EPS|\n",
            "+------+-------+-----+\n",
            "|  AAPL|  175.5| 6.16|\n",
            "|  GOOG|2850.25|145.6|\n",
            "|  MSFT|  425.8| 11.8|\n",
            "|  TSLA|  245.3| 3.65|\n",
            "|  AMZN|  186.8| 4.12|\n",
            "|  META|  567.9|20.35|\n",
            "|  NVDA| 890.45|12.05|\n",
            "+------+-------+-----+\n",
            "\n",
            "ðŸ”„ Transformation DAG built (lazy)\n",
            "ðŸš€ EXECUTING across cluster...\n",
            "+------+-------+-----+------------------+-----------+\n",
            "|Symbol|Price  |EPS  |pe_ratio          |pe_category|\n",
            "+------+-------+-----+------------------+-----------+\n",
            "|GOOG  |2850.25|145.6|19.575892857142858|Fair       |\n",
            "|META  |567.9  |20.35|27.906633906633903|Fair       |\n",
            "|AAPL  |175.5  |6.16 |28.490259740259738|Fair       |\n",
            "|MSFT  |425.8  |11.8 |36.08474576271186 |Overvalued |\n",
            "|AMZN  |186.8  |4.12 |45.33980582524272 |Overvalued |\n",
            "|TSLA  |245.3  |3.65 |67.2054794520548  |Overvalued |\n",
            "|NVDA  |890.45 |12.05|73.89626556016597 |Overvalued |\n",
            "+------+-------+-----+------------------+-----------+\n",
            "\n",
            "ðŸ“ˆ Portfolio Summary:\n",
            "+-----------+------------------+-----------+\n",
            "|pe_category|            avg_pe|stock_count|\n",
            "+-----------+------------------+-----------+\n",
            "|       Fair|25.324262168012165|          3|\n",
            "| Overvalued| 55.63157415004384|          4|\n",
            "+-----------+------------------+-----------+\n",
            "\n",
            "ðŸ’¾ Results written to stocks_pe_analysis/\n",
            "âœ… Spark cluster shutdown.\n"
          ]
        }
      ]
    }
  ]
}